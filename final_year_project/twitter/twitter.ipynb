{"cells":[{"cell_type":"code","source":["1+1 \n#sanity check"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["spark\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import tweepy\nfrom tweepy import OAuthHandler\nimport json\nimport numpy as np\nimport pandas as pd\nfrom kafka import SimpleProducer, KafkaClient\nfrom kafka import KafkaProducer\n\n\nconsumer_key = 'xxx'\nconsumer_secret = 'xxx'\naccess_token = 'xxx'\naccess_secret = 'xxx'\n\n \nauth = OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_secret)\n \napi = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#get a single tweet\n#get tweets from own timeline\nfor status in tweepy.Cursor(api.home_timeline).items(10):\n    print(status.text) "],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["def process_or_store(tweet):\n    print(json.dumps(tweet))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["import json\nfor status in tweepy.Cursor(api.home_timeline).items(10):\n    # Process a single status\n    process_or_store(status._json) \n    "],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#trends= api.trends_available()\n#print trends\napi.trends_available()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["query = '#ObamaFarewell'\ncursor = tweepy.Cursor(api.search, q=query, lang=\"en\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["import time\nfor page in cursor.pages(2):\n  tweets = []\nfor item in page:\n  TW= tweets.append(item._json)\n  print(TW)\n  time.sleep(2)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from tweepy.streaming import StreamListener\nfrom tweepy import OAuthHandler, Stream\n#from elasticsearch import Elasticsearch\nfrom datetime import datetime\nfrom textblob import TextBlob\n\n\nauth = OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_secret)\n\n\n\"\"\"pk = sc.textFile(\"/FileStore/tables/7f8jfm6e1485266500976/service.cert\")\nct = sc.textFile(\"/FileStore/tables/7f8jfm6e1485266500976/service.key\")\nca = sc.textFile(\"/FileStore/tables/e2n5myz11485269460096/ca.pem\")\n\"\"\"\n#es = Elasticsearch([elasticsearch_uri e.g  http://192.168.99.100:49156])\n\naces = []\nthor= {}\n\nclass Listener(StreamListener):\n  \n  def on_data(self, data):\n      \n    all_data= json.loads(data)\n    \n    tweet= all_data[\"text\"].encode('utf-8')\n    timestamp=all_data[\"timestamp_ms\"].encode('utf-8')\n    ## https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior\n    timestamp_clean = datetime.strptime(all_data[\"created_at\"].replace(\"+0000 \",\"\"), \"%a %b %d %H:%M:%S %Y\").isoformat()\n    created = all_data[\"created_at\"].encode('utf-8')\n    user = all_data[\"user\"][\"screen_name\"].encode('utf-8')\n    followers = all_data[\"user\"][\"followers_count\"]\n    location = all_data[\"user\"][\"location\"]\n    geo= all_data[\"geo\"]\n    \n    \n    \n    \n    blob= TextBlob(tweet.decode('utf-8'))\n    if blob.sentiment.polarity < 0:\n      sentiment = \"negative\"\n    elif blob.sentiment.polarity == 0:\n      sentiment = \"neutral\"\n    else:\n      sentiment = \"positive\"\n    \n    kz= [tweet, user]\n    thor={'timestamp': timestamp_clean,\n         'created_at': created,\n         'tweet_text': tweet,\n         'screen_name': user,\n         'followers': followers,\n         'location': location,\n         'sentiment_analysis': sentiment\n        \n         }\n    \n    #send data to elasticsearch\n    \"\"\"es.create(index= \"idx_twp\",\n                      doc_type= \"twitter_twp\",\n                      body= thor\n                     )\"\"\"\n    \n    #hammer =pd.DataFrame.from_dict(thor, orient='index')\n    \n    \n    \"\"\"from kafka import SimpleProducer, KafkaClient\n    from kafka import KafkaConsumer, KafkaProducer\n    \n    producer = KafkaProducer(bootstrap_servers= ['twitter-fyp.acehanks-7a98.aivencloud.com:27954'],\n                              security_protocol=\"SSL\",\n                              ssl_keyfile= pk,\n                              ssl_certfile= ct,\n                              ssl_check_hostname=True,\n                              ssl_cafile= ca,\n                              api_version=(0,10)\n                              )\n    #kafka_prod = KafkaProducer(bootstrap_servers='dogsled-01.srvs.cloudkafka.com')\n    producer.send('twitter-fyp', b'some_message_bytes' ).get(timeout=60)\n    producer.flush()\n\n    for msg in thor:\n      #kafka_prod.send('twitter-fyp', b'str(json.dumps(thor))' )\n      #kafka_prod.send('vj90-twitter', str(json.dumps(thor)))\n      producer.send('twitter-fyp', str(thor) )\n      \n      print str(json.dumps(thor))\"\"\"\n      \n    #heap= sc.parallelize(thor)\n    #heapDF= heap.map(lambda x: (x, )).toDF()\n    \n    \n    \n    \n    \n    #ready = spark.createDataFrame(aces)\n    #print all_data\n    print thor\n    print\n    return True #not to stop stream\n    \n#start stream with tags #syria and #iraq and perform sentiment analaysis  \nstream = Stream(auth, Listener())\nstream.filter(track=['syria', 'iraq'])\nstream.sample()\n\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["print thor"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":12}],"metadata":{"name":"twitter","notebookId":845356211472225},"nbformat":4,"nbformat_minor":0}
