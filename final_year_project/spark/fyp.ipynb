{"cells":[{"cell_type":"markdown","source":["![spark](http://spark.apache.org/images/spark-logo-trademark.png)"],"metadata":{}},{"cell_type":"code","source":["\n#ACCESSY_KEY_ID = \"xxxx\"\n#SECERET_ACCESS_KEY = \"xxxxx\" \n\n#mounts_list = [\n#{'bucket':'dannyfyp', 'mount_folder':'/mnt/apacheLog'}\n#]\n\n# Setup AWS configuration\nimport urllib\nACCESS_KEY = \"xxx\"\nSECRET_KEY = \"xxxx\"\nENCODED_SECRET_KEY = urllib.quote(SECRET_KEY, \"\")\nAWS_BUCKET_NAME = \"dannyfyp\"\nMOUNT_NAME = \"apacheLogs\"\n\n\n# Mount S3 bucket\ntry:\n  dbutils.fs.mount(\"s3n://%s:%s@%s/\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n  #dbutils.fs.unmount(\"/mnt/apacheLogs\")\n  \nexcept Exception:\n  pass\n\n\n\n\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["1+1\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["inputPath= \"/mnt/apacheLogs\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/mnt/apacheLogs\"))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["llk= spark.read.text(\"/mnt/apacheLogs\")\nllk.count()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#llk.show()\ndisplay(llk.limit(10))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql.functions import split, regexp_extract\nfrom pyspark.sql.types import *\n\nsplit_df =llk.select(#regexp_extract('value', r'^([^\\s]+\\s)', 1).alias('a'),\n                     #regexp_extract('value', r'^(\\w+)', 1).alias('b'),\n                     regexp_extract('value', r'^.*(\\d+\\.\\d+\\.\\d+\\.\\d+)', 1).alias('host'),\n                     #regexp_extract('value', r'.*^([^\\s]+\\s)', 1).alias('host'),\n                     regexp_extract('value', r'^.*\\[(\\d\\d/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]', 1).alias('timestamp'),\n                     regexp_extract('value', r'^.*\"\\w+\\s+([^\\s]+)\\s+HTTP.*\"', 1).alias('path'),\n                     regexp_extract('value', r'^.*\"\\s+([^\\s]+)', 1).cast('integer').alias('status'),\n                     regexp_extract('value', r'^.*\\s+(\\d+)$', 1).cast('integer').alias('content_size'))\nsplit_df.show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["display(split_df.limit(10))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["bad_rows_df = split_df.filter(split_df['host'].isNull() |\n                              split_df['timestamp'].isNull() |\n                              split_df['path'].isNull() |\n                              split_df['status'].isNull() |\n                             split_df['content_size'].isNull())\nbad_rows_df.count()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.sql.functions import col, sum\n\ndef count_null(col_name):\n  return sum(col(col_name).isNull().cast('integer')).alias(col_name)\n\n# Build up a list of column expressions, one per column.\n#\n# This could be done in one line with a Python list comprehension, but we're keeping\n# it simple for those who don't know Python very well.\nexprs = []\nfor col_name in split_df.columns:\n  exprs.append(count_null(col_name))\n\n# Run the aggregation. The *exprs converts the list of expressions into\n# variable function arguments.\nsplit_df.agg(*exprs).show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Replace all null content_size values with 0.\ncleaned_df = split_df.na.fill({'content_size': 0})"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["exprs = []\nfor col_name in cleaned_df.columns:\n  exprs.append(count_null(col_name))\n\ncleaned_df.agg(*exprs).show()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["month_map = {\n  'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7,\n  'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12\n}\n\ndef parse_clf_time(s):\n    \"\"\" Convert Common Log time format into a Python datetime object\n    Args:\n        s (str): date and time in Apache time format [dd/mmm/yyyy:hh:mm:ss (+/-)zzzz]\n    Returns:\n        a string suitable for passing to CAST('timestamp')\n    \"\"\"\n    # NOTE: We're ignoring time zone here. In a production application, you'd want to handle that.\n    return \"{0:04d}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}\".format(\n      int(s[7:11]),\n      month_map[s[3:6]],\n      int(s[0:2]),\n      int(s[12:14]),\n      int(s[15:17]),\n      int(s[18:20])\n    )\n\nu_parse_time = udf(parse_clf_time)\n\nlogs_df = cleaned_df.select('*', u_parse_time(split_df['timestamp']).cast('timestamp').alias('time')).drop('timestamp')\ntotal_log_entries = logs_df.count()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(logs_df.limit(10))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["logs_df.cache()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["status_to_count_df =(logs_df\n                     .groupBy('status')\n                     .count()\n                     .sort('status')\n                     .cache())"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["display(status_to_count_df)\n"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["host_sum_df =(logs_df\n              .groupBy('host')\n              .count()\n             .sort('count', ascending=False))\n\ndisplay(host_sum_df)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["paths_df = (logs_df\n            .groupBy('path')\n            .count()\n            .sort('count', ascending=False)).take(2)\ndisplay(paths_df)\n\n\n  "],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["from pyspark.sql.functions import dayofmonth\n\nday_to_host_pair_df = logs_df.select('host', dayofmonth('time').alias('day'))\nday_group_hosts_df = day_to_host_pair_df.dropDuplicates()\n\ndaily_hosts_df = day_group_hosts_df.groupBy('day').count().withColumnRenamed('count', 'uniqueHost').sort('count', ascending= False).cache()\n\nprint 'Unique hosts per day:'\ndaily_hosts_df.sort('day', ascending= True).show(30, False)\n\ndays_with_hosts, hosts = [list(r) for r in zip(*daily_hosts_df.collect())]\n#0.27s\n\n\n  \nprint(days_with_hosts)\nprint(hosts)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["display(daily_hosts_df)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["total_req_per_day_df = logs_df.select('host', dayofmonth('time').alias('day')).groupBy('day').count()\n\navg_daily_req_per_host_df = (\n  total_req_per_day_df\n  .join(daily_hosts_df, 'day', 'left_outer')\n  .select('day', (col('count') / col('uniqueHost'))\n          .cast('integer')\n          .alias('avg_reqs_per_host_per_day'))\n  .cache()\n)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["print(\"Average daily hosts are given by:\")\navg_daily_req_per_host_df.show()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["from pyspark.sql.functions import *\nrrStream= (\n  spark    \n    .readStream                       \n    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n    .text(inputPath)\n)\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["rrStream.isStreaming"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["display(rrStream)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["logStream= (\nrrStream.select(#regexp_extract('value', r'^([^\\s]+\\s)', 1).alias('a'),\n                     #regexp_extract('value', r'^(\\w+)', 1).alias('b'),\n                     regexp_extract('value', r'^.*(\\d+\\.\\d+\\.\\d+\\.\\d+)', 1).alias('host'),\n                     #regexp_extract('value', r'.*^([^\\s]+\\s)', 1).alias('host'),\n                     regexp_extract('value', r'^.*\\[(\\d\\d/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]', 1).alias('time_stamp'),\n                     regexp_extract('value', r'^.*\"\\w+\\s+([^\\s]+)\\s+HTTP.*\"', 1).alias('path'),\n                     regexp_extract('value', r'^.*\"\\s+([^\\s]+)', 1).cast('integer').alias('status'),\n                     regexp_extract('value', r'^.*\\s+(\\d+)$', 1).cast('integer').alias('content_size'))\n)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["logStream.dtypes"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["display(logStream)\n"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["status_to_count_df =(logStream\n                     .groupBy('status')\n                     .count()\n                     .sort('count', ascending= False)\n                   )"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["display(status_to_count_df)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["host_sum_dfz =(logStream\n              .groupBy('host')\n              .count()\n             .sort('count', ascending=False))\n\ndisplay(host_sum_dfz)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["#split_df['timestamp']).cast('timestamp').alias('time')).drop('timestamp')\n#fireServiceCallsTsDF = fireServiceCallsDF \\\n  #.withColumn('CallDateTS', unix_timestamp(fireServiceCallsDF['CallDate'], from_pattern1).cast(\"timestamp\")) \\\n  #.drop('CallDate')\n\nfrom_pattern = 'dd/MMM/yyyy hh:mm:ss z'\n#to_pattern2 = 'yyyy-MMM-dd hh:mm:ss aa'\nto_pattern = 'yyyy-MM-dd hh:mm'\n\n\n#logStream= logStream.withColumn('Timestamp', unix_timestamp(logStream['time_stamp'], from_pattern2).cast(\"timestamp\"))\nlogStreamz= logStream.withColumn('Timestamp', from_unixtime(unix_timestamp(logStream['time_stamp'], from_pattern), to_pattern))\n"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["#trying to fix time_stamp to unix timestamp\ndisplay(logStreamz)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["\"\"\"\nstreamingCountsDF = (                 \n  rrStream\n    .groupBy(\n      \"host\", \n      window(rrStream.timestamp, \"1 hour\"))\n    .count().select(\"host\", date_format(\"window.end\", \"MMM-dd HH:mm\").alias(\"time\"), \"count\").orderBy(\"time\", \"host\")\n  \n  )\n  \"\"\""],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["\"\"\"import urllib2\n\nfor r in ip_add:\n  url = 'http://freegeoip.net/json/' + 'r'\n\n  k= urllib2.urlopen(url).read()\n  print(k)\n\n\"\"\"\n\nipz_df= logs_df.select('host').dropDuplicates()\n\nipz_df.show()\n"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["import urllib2\nfrom pyspark.sql import SQLContext, Row\nipz= logs_df.select('host').dropDuplicates().rdd\n\ndef getIP(ip):\n  url = 'http://freegeoip.net/csv/' + ip\n  str = urllib2.urlopen(url).read()\n  cca2 = str.split(\",\")[1]\n  country= str.split(\",\")[2]\n  Lat= str.split(\",\")[8]\n  Long= str.split(\",\")[9]\n  \n  #deeds= [cca2, country, Lat, Long]\n  #deeds= {'cca2': cca2, 'country':country, 'latitude': Lat, 'longitude': Long}\n  return cca2\n\nmappedIPs = ipz.map(lambda x: (x[0], getIP(x[0])))\n\nmappedIPs.take(10)\n\n#ipz.map(lambda x: x[0])"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["\n# mappedIP2: contains the IP address and CCA2 codes\n# The schema is encoded in a string.\nschemaString = \"ip cca2\"\nfields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\nschema = StructType(fields)\n\n# Create DataFrame with schema\nmappedIP2 = sqlContext.createDataFrame(mappedIPs, schema)\n\nmappedIP2.show()"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["mappedIP2.groupBy(\"ip\", \"cca2\").count().show()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["mappedIP2.registerTempTable(\"jayz\")"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["%sql select * from jayz limit 10;"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["import urllib2\nfrom pyspark.sql import SQLContext, Row\nfrom pyspark.sql.types import *\n\n# Obtain the unique agents from the accesslog table\nipaddresses = sqlContext.sql(\"select distinct ip1 from accesslog where ip1 is not null\").rdd\n\n# Convert None to Empty String\ndef xstr(s): \n  if s is None: \n    return '' \n  return str(s)\n\n# getCCA2: Obtains two letter country code based on IP address\ndef getCCA2(ip):\n  # Obtain CCA2 code from FreeGeoIP\n  url = 'http://freegeoip.net/csv/' + ip\n  str = urllib2.urlopen(url).read()\n  cca2 = str.split(\",\")[1]\n  \n  # return\n  return cca2\n\n# Loop through distinct IP addresses and obtain two-letter country codes\nmappedIPs = ipaddresses.map(lambda x: (x[0], getCCA2(x[0])))\n\n# mappedIP2: contains the IP address and CCA2 codes\n# The schema is encoded in a string.\nschemaString = \"ip cca2\"\nfields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\nschema = StructType(fields)\n\n# Create DataFrame with schema\nmappedIP2 = sqlContext.createDataFrame(mappedIPs, schema)\n\n# Obtain the Country Codes \nfields = sc.textFile(\"/mnt/tardis6/countrycodes/\").map(lambda l: l.split(\",\"))\ncountrycodes = fields.map(lambda x: Row(cn=x[0], cca2=x[1], cca3=x[2]))\n\n# Country Codes DataFrame:\n#   Create DataFrame (inferring schema using reflection)\ncountryCodesDF = sqlContext.createDataFrame(countrycodes)\n\n# Join countrycodes with mappedIPsDF so we can have IP address and three-letter ISO country codes\nmappedIP3 = mappedIP2 \\\n  .join(countryCodesDF, mappedIP2.cca2 == countryCodesDF.cca2, \"left_outer\") \\\n  .select(mappedIP2.ip, mappedIP2.cca2, countryCodesDF.cca3, countryCodesDF.cn)\n  \n# Register the mapping table\nmappedIP3.registerTempTable(\"mappedIP3\")"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["![amazon](http://pjlewis-wordpress.s3-eu-west-1.amazonaws.com/wp-content/uploads/2015/09/amazon-web-services-logo.png)"],"metadata":{}},{"cell_type":"markdown","source":["spark intergration with kafka 10.\nSee link:https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"],"metadata":{}},{"cell_type":"code","source":["\"\"\"\nval kafka = spark.readStream\n  .format(\"kafka\")\n  .option(\"kafka.bootstrap.servers\", \"host:port\")\n  .option(\"subscribe\", \"topic\")\n  .option(\"startingOffsets\", \"latest\")\n  .load()\n  \n  # Subscribe to 1 topic\nds1 = spark\n  .readStream()\n  .format(\"kafka\")\n  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n  .option(\"subscribe\", \"topic1\")\n  .load()\nds1.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n\n# Subscribe to multiple topics\nds2 = spark\n  .readStream\n  .format(\"kafka\")\n  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n  .option(\"subscribe\", \"topic1,topic2\")\n  .load()\nds2.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n\n# Subscribe to a pattern\nds3 = spark\n  .readStream()\n  .format(\"kafka\")\n  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n  .option(\"subscribePattern\", \"topic.*\")\n  .load()\nds3.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n\"\"\""],"metadata":{},"outputs":[],"execution_count":48}],"metadata":{"name":"fyp","notebookId":1363072632225895},"nbformat":4,"nbformat_minor":0}
